<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CUDA Learning Outline</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 20px;
        color: #333;
      }
      h1,
      h2,
      h3 {
        color: #0056b3;
      }
      h2 {
        border-bottom: 2px solid #0056b3;
        padding-bottom: 5px;
      }
      ul {
        list-style-type: none;
        padding-left: 0;
      }
      li {
        margin-bottom: 10px;
      }
      a {
        color: #0056b3;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
      code {
        background-color: #f4f4f4;
        padding: 2px 4px;
        border-radius: 4px;
        color: #d63384;
      }
      pre {
        background-color: #f4f4f4;
        padding: 10px;
        border-radius: 4px;
        overflow: auto;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Learning CUDA</h1>

      <h2>1. CUDA Programming Model</h2>
      <div class="container">
        <img src="./img/cuda/id.jpg" alt="GPU vs CPU" />
        <p>Figure 1: The methods of calculating id</p>
      </div>

      <h3>You should know the following concepts:</h3>
      <ol>
        <li>
          <strong>GridIdx</strong>: The unique index of a grid that identifies
          which grid is currently executing. It is calculated as:
          <code>gridIdx.x + gridDim.x * gridIdx.y</code> (for 2D grids).
        </li>
        <li>
          <strong>GridDim</strong>: Represents the total number of blocks in the
          grid. It can be accessed via <code>gridDim.x</code> and
          <code>gridDim.y</code>.
        </li>
        <li>
          <strong>blockIdx</strong>: The unique index of the block within the
          grid, used to identify which block is executing.
        </li>
        <li>
          <strong>blockDim</strong>: The dimensions of each block, providing the
          number of threads in each block. Accessed via
          <code>blockDim.x</code> and <code>blockDim.y</code>.
        </li>
        <li>
          <strong>ThreadIdx</strong>: The unique index of the thread within its
          block. It is used to identify which thread is executing within the
          block.
        </li>
      </ol>
      <h2>Memory Management</h2>
      <h3>1. <span class="method">cudaMalloc</span></h3>
      <p>
        <code>cudaMalloc</code> allocates memory on the GPU (device). This is
        the first step to store data on the device before performing
        computations.
      </p>
      <pre><code>cudaError_t cudaMalloc(void **devPtr, size_t size);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li>
          <strong>devPtr</strong>: Pointer to the allocated memory on the
          device.
        </li>
        <li>
          <strong>size</strong>: The size of the allocated memory (in bytes).
        </li>
      </ul>

      <h3>2. <span class="method">cudaMemset</span></h3>
      <p>
        <code>cudaMemset</code> initializes or sets memory on the device to a
        specific value. It is typically used to initialize device memory to zero
        or another byte-level value.
      </p>
      <pre><code>cudaError_t cudaMemset(void *devPtr, int value, size_t count);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li><strong>devPtr</strong>: Pointer to the device memory.</li>
        <li><strong>value</strong>: The value to set in memory (usually 0).</li>
        <li><strong>count</strong>: Number of bytes to set.</li>
      </ul>

      <h3>3. <span class="method">cudaMemcpy</span></h3>
      <p>
        <code>cudaMemcpy</code> copies data between the host (CPU) and device
        (GPU). It can also be used for copying data between different memory
        regions within the GPU.
      </p>
      <pre><code>cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li><strong>dst</strong>: Destination pointer (host or device).</li>
        <li><strong>src</strong>: Source pointer (host or device).</li>
        <li><strong>count</strong>: Number of bytes to copy.</li>
        <li>
          <strong>kind</strong>: The direction of the copy:
          <ul>
            <li><code>cudaMemcpyHostToDevice</code></li>
            <li><code>cudaMemcpyDeviceToHost</code></li>
            <li><code>cudaMemcpyDeviceToDevice</code></li>
            <li><code>cudaMemcpyHostToHost</code></li>
          </ul>
        </li>
      </ul>

      <h3>4. <span class="method">cudaFree</span></h3>
      <p>
        <code>cudaFree</code> releases or deallocates memory previously
        allocated on the GPU with <code>cudaMalloc</code>.
      </p>
      <pre><code>cudaError_t cudaFree(void *devPtr);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li>
          <strong>devPtr</strong>: Pointer to the memory to be freed on the
          device.
        </li>
      </ul>
      <img
        style="width: 800px"
        src="./img/cuda/vector_add.png"
        alt="Vector add"
      />
      <h3>5. <span class="method">cudaMallocManaged</span></h3>
      <p>
        <code>cudaMallocManaged</code> allocates <em>unified memory</em> that
        can be accessed by both the host and device without explicit copying.
        This simplifies memory management, as data can be transparently
        transferred between CPU and GPU.
      </p>
      <pre><code>cudaError_t cudaMallocManaged(void **devPtr, size_t size);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li>
          <strong>devPtr</strong>: Pointer to the allocated unified memory.
        </li>
        <li><strong>size</strong>: The size of the allocated memory.</li>
      </ul>

      <h3>6. <span class="method">cudaMemPrefetchAsync</span></h3>
      <p>
        <code>cudaMemPrefetchAsync</code> prefetches memory to a specified
        device asynchronously. It's used for managing the placement of managed
        memory, ensuring that the data is available on the desired device (CPU
        or GPU).
      </p>
      <pre><code>cudaError_t cudaMemPrefetchAsync(void *devPtr, size_t count, int device, cudaStream_t stream);</code></pre>
      <p><strong>Parameters:</strong></p>
      <ul>
        <li><strong>devPtr</strong>: Pointer to the managed memory.</li>
        <li><strong>count</strong>: Number of bytes to prefetch.</li>
        <li>
          <strong>device</strong>: The device to prefetch to (0 for host, device
          ID for GPU).
        </li>
        <li>
          <strong>stream</strong>: The CUDA stream for asynchronous execution
          (can be 0 for default stream).
        </li>
      </ul>
      <img
        style="width: 800px"
        src="./img/cuda/unified_memory.png"
        alt="Unified Memory"
      />
      <h3>7. <span class="method">cudaDeviceSynchronize</span></h3>
      <p>
        <code>cudaDeviceSynchronize</code> waits for the device to complete all
        preceding tasks. This is commonly used after launching kernels or memory
        operations to ensure that the device is idle before proceeding.
      </p>
      <pre><code>cudaError_t cudaDeviceSynchronize(void);</code></pre>

      <h3>8.<span class="method">cudaHostAlloc</span></h3>
      <p>
        <code
          >cudaHostAlloc ( void** pHost, size_t size, unsigned int flags )</code
        >
        Allocates page-locked memory on the host.
      </p>
      <img
        style="width: 800px"
        src="./img/cuda/pinned_memory.png"
        alt="pinned Memory"
      />
      <h2>Performance Analysis</h2>
      <h3>NVIDIA Nsight Systems</h3>
      <li>
        <code>nsys profile --stats=true ./ex</code>#generate a analysis report
      </li>
      <li><code>nsys-ui</code> open ui to see the details of report</li>
      <h3>NVIDIA Nsight Compute</h3>
      <li><code>ncu --metrics "metric_name" ./your_cuda_application </code></li>
      <li><code>ncu-ui</code></li>
      <h2>Cooperative Groups</h2>
      <img
        style="width: 800px"
        src="./img/cuda/cooperative_groups.png"
        alt="pinned Memory"
      />
      <img
        style="width: 800px"
        src="./img/cuda/cooperative_group.png"
        alt="pinned Memory"
      />
      <p>
        协作组的引入简化了并行编程的复杂性，提供了更灵活和高效的线程同步与协调机制，特别适用于需要不同规模的线程组共同完成任务的场景。
      </p>
      <li>灵活的线程分组和同步: 协作组提供了对部分线程进行分组并同步的能力</li>
      <li>
        简化的线程间通信：允许在更小范围内控制同步操作，比如部分线程同步，或者跨越不同线程块的同步操作。
      </li>
      <li>
        跨线程块同步 (Grid-Wide Synchronization) :
        通常CUDA只能在一个线程块内部同步,
        多个线程块之间的同步需要借助CPU,让网格中的所有线程进行同步,提升了并行计算任务的灵活性。
      </li>
      <li>简化的代码编写</li>
      <li>性能优化</li>
      <h3>协作组的常见操作</h3>
      <li>
        <code>cooperative_groups::this_thread_block()</code
        >#返回当前线程块的协作组
      </li>
      <li>
        <code>cooperative_groups::thread_group::split()</code
        >#将线程块进一步划分为多个线程子组
      </li>
      <li><code>group.sync()</code>#对协作组内的线程进行同步</li>
      <li>
        <code>cooperative_groups::grid_group</code
        >#用于管理网格范围的协作，支持跨越多个线程块的同步操作。
      </li>
    </div>

    <div class="container">
      <a href="index.html" class="back-link">Back to Index</a>
    </div>
  </body>
</html>
