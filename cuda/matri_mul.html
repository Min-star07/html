<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDA Matrix Multiplication Code Explanation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            color: #333;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        h2 {
            border-bottom: 2px solid #0056b3;
            padding-bottom: 5px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            color: #d63384;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow: auto;
        }
    </style>
</head>
<body>
    <h1>CUDA Matrix Multiplication Code Explanation</h1>

    <p>This program demonstrates matrix multiplication using CUDA (Compute Unified Device Architecture) to perform the computation on a GPU. Below is a detailed explanation of the code:</p>

    <h2>1. Variable Declarations and Error Checking</h2>
    <p>
        The code begins by including necessary headers like <code>&lt;stdio.h&gt;</code> for standard I/O and <code>&lt;time.h&gt;</code> for timing purposes.
    </p>
    <pre><code>#include &lt;stdio.h&gt;
#include &lt;time.h&gt;
</code></pre>

    <p>The macro <code>cudaCheckErrors</code> is defined to check for CUDA errors after each API call.</p>
    <pre><code>#define cudaCheckErrors(msg) { ... }</code></pre>

    <h2>2. Constants and Matrix Initialization</h2>
    <p>
        The program defines the matrix size, block size, and some constant values for matrices <code>A</code> and <code>B</code>. These are defined as constants.
    </p>
    <pre><code>const int DSIZE = 8192;
const int block_size = 32;
const float A_val = 3.0f;
const float B_val = 2.0f;</code></pre>

    <h2>3. Matrix Multiplication Kernel</h2>
    <p>
        The CUDA kernel <code>mmul</code> performs matrix multiplication using shared memory to speed up access within each block. It multiplies matrix <code>A</code> by matrix <code>B</code>, and stores the result in matrix <code>C</code>.
    </p>
    <pre><code>__global__ void mmul(const float *A, const float *B, float *C, int ds) {
    __shared__ float As[block_size][block_size];
    __shared__ float Bs[block_size][block_size];

    int idx = threadIdx.x + blockDim.x * blockIdx.x;
    int idy = threadIdx.y + blockDim.y * blockIdx.y;

    if ((idx &lt; ds) && (idy &lt; ds)) {
        float temp = 0;
        for (int i = 0; i &lt; ds / block_size; i++) {
            As[threadIdx.y][threadIdx.x] = A[idy * ds + (i * block_size + threadIdx.x)];
            Bs[threadIdx.y][threadIdx.x] = B[(i * block_size + threadIdx.y) * ds + idx];
            __syncthreads();

            for (int k = 0; k &lt; block_size; k++)
                temp += As[threadIdx.y][k] * Bs[k][threadIdx.x];

            __syncthreads();
        }
        C[idy * ds + idx] = temp;
    }
}</code></pre>

    <p>Key points:</p>
    <ul>
        <li>Shared memory is used to store sub-matrices from <code>A</code> and <code>B</code> into <code>As</code> and <code>Bs</code>.</li>
        <li>The kernel calculates the dot product of a row from <code>A</code> and a column from <code>B</code>, and stores the result in <code>C</code>.</li>
        <li>Synchronization is achieved using <code>__syncthreads()</code> to ensure all threads in the block have loaded their data before computation.</li>
    </ul>

    <h2>4. Host Code (CPU)</h2>
    <p>
        The host code allocates memory for the matrices on both the CPU and the GPU, initializes them, and then transfers the data to the GPU. It also manages kernel launches and error handling.
    </p>
    <pre><code>// Allocate memory and initialize matrices
h_A = new float[DSIZE * DSIZE];
h_B = new float[DSIZE * DSIZE];
h_C = new float[DSIZE * DSIZE];

for (int i = 0; i &lt; DSIZE * DSIZE; i++) {
    h_A[i] = A_val;
    h_B[i] = B_val;
    h_C[i] = 0;
}</code></pre>

    <p>
        It then transfers the data to the GPU using <code>cudaMemcpy</code> and launches the kernel:
    </p>
    <pre><code>cudaMalloc(&d_A, DSIZE * DSIZE * sizeof(float));
cudaMalloc(&d_B, DSIZE * DSIZE * sizeof(float));
cudaMalloc(&d_C, DSIZE * DSIZE * sizeof(float));

cudaMemcpy(d_A, h_A, DSIZE * DSIZE * sizeof(float), cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, DSIZE * DSIZE * sizeof(float), cudaMemcpyHostToDevice);

// Launch kernel
dim3 block(block_size, block_size);
dim3 grid((DSIZE + block.x - 1) / block.x, (DSIZE + block.y - 1) / block.y);
mmul grid, block>>>(d_A, d_B, d_C, DSIZE);</code></pre>

    <h2>5. Timing and Validation</h2>
    <p>
        The host measures the time for memory allocation, kernel execution, and data transfer. After the kernel completes, the results are copied back to the CPU memory and checked for correctness.
    </p>
    <pre><code>cudaMemcpy(h_C, d_C, DSIZE * DSIZE * sizeof(float), cudaMemcpyDeviceToHost);
for (int i = 0; i &lt; DSIZE * DSIZE; i++) {
    if (h_C[i] != A_val * B_val * DSIZE) {
        printf("Mismatch at index %d\n", i);
        return -1;
    }
}</code></pre>

    <h2>6. Output</h2>
    <p>If the results are correct, the program prints "Success!" and the timing information.</p>
    <pre><code>
